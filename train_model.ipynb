{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model training\n",
                "This notebook initializes dataset, defines the model, and implements training function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "-G4bZhehAuXx"
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from PIL import Image\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "from torchvision.models import resnet18\n",
                "\n",
                "from torch.utils.data import DataLoader, Subset\n",
                "from torchvision.datasets import ImageFolder\n",
                "from torchvision.transforms import Compose, ToTensor, Resize, CenterCrop, RandomCrop, RandomHorizontalFlip, ColorJitter, Normalize\n",
                "from sklearn.model_selection import train_test_split\n",
                "from tqdm import tqdm\n",
                "\n",
                "import os\n",
                "import random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Data Loading\n",
                "We load the RealWaste dataset and divide it into training, validation and test subsets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "KFnX37_JHg36"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total images: 4752\n",
                        "Train: 3798, Val: 475, Test: 479\n"
                    ]
                }
            ],
            "source": [
                "data_dir = \"dataset/RealWaste\"\n",
                "\n",
                "total_images = sum([len(files) for _, _, files in os.walk(data_dir)])\n",
                "print(f\"Total images: {total_images}\")\n",
                "\n",
                "dataset = ImageFolder(root=data_dir, transform=ToTensor())\n",
                "\n",
                "# Get indices for each class\n",
                "class_indices = {i: [] for i in range(len(dataset.classes))}\n",
                "for idx, (_, label) in enumerate(dataset.samples):\n",
                "    class_indices[label].append(idx)\n",
                "\n",
                "# Split into train/val/test maintaining class balance\n",
                "train_indices = []\n",
                "val_indices = []\n",
                "test_indices = []\n",
                "\n",
                "for label, indices in class_indices.items():\n",
                "    # First split into train and temp (val+test)\n",
                "    train_idx, temp_idx = train_test_split(\n",
                "        indices, train_size=0.8, random_state=42, stratify=[label]*len(indices)\n",
                "    )\n",
                "    # Then split temp into val and test\n",
                "    val_idx, test_idx = train_test_split(\n",
                "        temp_idx, train_size=0.5, random_state=42, stratify=[label]*len(temp_idx)\n",
                "    )\n",
                "    train_indices.extend(train_idx)\n",
                "    val_indices.extend(val_idx)\n",
                "    test_indices.extend(test_idx)\n",
                "\n",
                "# Create subsets\n",
                "train_dataset = Subset(dataset, train_indices)\n",
                "val_dataset = Subset(dataset, val_indices)\n",
                "test_dataset = Subset(dataset, test_indices)\n",
                "\n",
                "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preprocessing\n",
                "We apply necessary transformations such as resizing and normalization to prepare the images for model training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "gQhZvf-kJ_B6"
            },
            "outputs": [],
            "source": [
                "# Normalization statistics\n",
                "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
                "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
                "\n",
                "# Augmentation for training\n",
                "train_transforms = Compose([\n",
                "    Resize(256),                      # Resize to 256\n",
                "    RandomCrop(224),                  # Random crop to 224x224\n",
                "    RandomHorizontalFlip(p=0.5),      # Horizontal flip\n",
                "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1), # Color jitter\n",
                "    ToTensor(),\n",
                "    Normalize(IMAGENET_MEAN, IMAGENET_STD)  # Normalization\n",
                "])\n",
                "\n",
                "# Transforms for val/test (no augmentation)\n",
                "val_test_transforms = Compose([\n",
                "    Resize(256),\n",
                "    CenterCrop(224),                  # Center crop\n",
                "    ToTensor(),\n",
                "    Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
                "])\n",
                "\n",
                "# Apply transforms\n",
                "train_dataset.dataset.transform = train_transforms\n",
                "val_dataset.dataset.transform = val_test_transforms\n",
                "test_dataset.dataset.transform = val_test_transforms\n",
                "\n",
                "# Create DataLoaders\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
                "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
                "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Model\n",
                "We define the model class WasteClassifier and use ResNet18 as the base model. We use cuda to run the model on the GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0X4MaaTbLVLR"
            },
            "outputs": [],
            "source": [
                "# define the model ensuring consistency with model.py\n",
                "class WasteClassifier(nn.Module):\n",
                "    def __init__(self, num_classes=9, weights='DEFAULT'):\n",
                "        super().__init__()\n",
                "        # Use weights instead of pretrained=True to avoid deprecation warnings\n",
                "        self.backbone = resnet18(weights=weights)\n",
                "        # replace the last layer\n",
                "        in_features = self.backbone.fc.in_features\n",
                "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.backbone(x)\n",
                "\n",
                "model = WasteClassifier(num_classes=9)\n",
                "if torch.cuda.is_available():\n",
                "    model = model.cuda()  # Move to GPU"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5. Handling Class Imbalance\n",
                "We use Weighted Cross-Entropy Loss to handle class imbalance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "nq7U3xe0QvE7"
            },
            "outputs": [],
            "source": [
                "# Calculate class weights\n",
                "class_counts = [len(indices) for indices in class_indices.values()]\n",
                "total_samples = sum(class_counts)\n",
                "class_weights = [total_samples / count for count in class_counts]\n",
                "\n",
                "# Convert to tensor\n",
                "weights_tensor = torch.FloatTensor(class_weights)\n",
                "if torch.cuda.is_available():\n",
                "    weights_tensor = weights_tensor.cuda()\n",
                "\n",
                "# Use weighted loss\n",
                "criterion = nn.CrossEntropyLoss(weight=weights_tensor)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 6. Model training\n",
                "We train the model for 25 epochs and save the best model weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "GVa2veLSQ5-U"
            },
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "EPOCHS = 25\n",
                "LEARNING_RATE = 0.001\n",
                "\n",
                "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
                "\n",
                "best_val_acc = 0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    # Train\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    train_correct = 0\n",
                "\n",
                "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS} - Train'):\n",
                "        if torch.cuda.is_available():\n",
                "            images, labels = images.cuda(), labels.cuda()\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        train_loss += loss.item()\n",
                "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
                "\n",
                "    train_acc = train_correct / len(train_dataset)\n",
                "\n",
                "    # validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    val_correct = 0\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for images, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{EPOCHS} - Val'):\n",
                "            if torch.cuda.is_available():\n",
                "                images, labels = images.cuda(), labels.cuda()\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "\n",
                "            val_loss += loss.item()\n",
                "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
                "\n",
                "    val_acc = val_correct / len(val_dataset)\n",
                "    scheduler.step(val_loss)\n",
                "\n",
                "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
                "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
                "\n",
                "    # save the best model\n",
                "    if val_acc > best_val_acc:\n",
                "        best_val_acc = val_acc\n",
                "        torch.save(model.state_dict(), 'pretrained/best_waste_model.pth')\n",
                "        print(f'New best model saved with accuracy: {val_acc:.4f}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
